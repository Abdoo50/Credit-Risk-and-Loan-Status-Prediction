{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":216379,"sourceType":"datasetVersion","datasetId":92906}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"text-align: center;\">\n    <img src=\"https://github.com/Abdoo50/Credit-Risk-and-Loan-Status-Prediction/raw/main/Financial%20Services%20Logo.png\" alt=\"Financial Services Project\"/>\n</p>\n\n\n**Context of the Problem**\n\nOne of the leading banks would like to predict bad customer while customer applying for loan. This model also called as PD Models (Probability of Default)\n\nCredit scoring is perhaps one of the most \"classic\" applications for predictive modeling, to predict whether or not credit extended to an applicant will likely result in profit or losses for the lending institution. There are many variations and complexities regarding how exactly credit is extended to individuals, businesses, and other organizations for various purposes (purchasing equipment, real estate, consumer items, and so on), and using various methods of credit (credit card, loan, delayed payment plan). But in all cases, a lender provides money to an individual or institution, and expects to be paid back in time with interest commensurate with the risk of default. Credit scoring is the set of decision models and their underlying techniques that aid lenders in the granting of consumer credit. These techniques determine who will get credit, how much credit they should get, and what operational strategies will enhance the profitability of the borrowers to the lenders. Further, they help to assess the risk in lending. Credit scoring is a dependable assessment of a personâ€™s credit worthiness since it is based on actual data.\n\nA lender commonly makes two types of decisions: first, whether to grant credit to a new applicant, and second, how to deal with existing applicants, including whether to increase their credit limits. In both cases, whatever the techniques used, it is critical that there is a large sample of previous customers with their application details, behavioral patterns, and subsequent credit history available. Most of the techniques use this sample to identify the connection between the characteristics of the consumers (annual income, age, number of years in employment with their current employer, etc.) and their subsequent history.\n\nTypical application areas in the consumer market include: credit cards, auto loans, home mortgages, home equity loans, mail catalog orders, and a wide variety of personal loan products.\n\n\n\nThe dataset contains the following columns:\n\n1. **age**: Age of the individual.\n2. **ed**: Education level.\n3. **employ**: Years of employment.\n4. **address**: Years at the current address.\n5. **income**: Annual income.\n6. **debtinc**: Debt-to-income ratio.\n7. **creddebt**: Credit card debt.\n8. **othdebt**: Other debts.\n9. **default**: Indicates if the individual has defaulted on a loan (1 for default, 0 for no default).","metadata":{}},{"cell_type":"markdown","source":"# Import Dependences","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np # for Statistical Operations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport xgboost as xgb\n\nimport statsmodels.formula.api as sm\nimport scipy.stats as stats\nimport pandas_profiling   #need to install using anaconda prompt (pip install pandas_profiling)\nfrom pandas_profiling import profile_report\n%matplotlib inline\nplt.rcParams['figure.figsize'] = 10, 7.5\nplt.rcParams['axes.grid'] = True\nplt.gray()\n\nfrom matplotlib.backends.backend_pdf import PdfPages\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom imblearn.over_sampling import SMOTE\nfrom patsy import dmatrices\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-11T17:03:10.379444Z","iopub.execute_input":"2024-01-11T17:03:10.380453Z","iopub.status.idle":"2024-01-11T17:03:10.401309Z","shell.execute_reply.started":"2024-01-11T17:03:10.380392Z","shell.execute_reply":"2024-01-11T17:03:10.399601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/bankloans/bankloans.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:34:55.831244Z","iopub.execute_input":"2024-01-11T16:34:55.832295Z","iopub.status.idle":"2024-01-11T16:34:55.863679Z","shell.execute_reply.started":"2024-01-11T16:34:55.832238Z","shell.execute_reply":"2024-01-11T16:34:55.862193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Data preprocessing is a critical step in the data analysis pipeline, involving the transformation of raw data into a format that is suitable for further analysis. The structure of data preprocessing can vary depending on the specific needs of your project, but it typically includes the following steps:\n\n\n1. **Data Cleaning**:\n   - Handling missing values by Using Predictive Modeling `LogisticRegression`.\n   - Identifying and correcting errors or inaccuracies in the data.\n   - Removing duplicate records.\n   - Detect Outliers and Capturing them\n   - **Handle Imbalanced Data using Resampling** the minority class in `default`: Creating additional samples for the minority class Using `SMOTE Method`.\n\n2. **Data Transformation**:\n   - Normalization: Scaling numerical data to a standard range.\n   - Standardization: Transforming data to have a mean of zero and a standard deviation of one.\n\n3. **Feature Engineering**:\n   - Creating new features from the existing data to improve the performance of machine learning models.\n   - Use Dimensionality Reduction techniques Such as:\n       - Feature Selection\n       - Feature Extraction: Includes interaction terms, polynomial features, and aggregations.","metadata":{}},{"cell_type":"code","source":"# Generate the profile report\nprofile = df.profile_report(title='Bank Loans Data Report')\n\n# Save the report to an HTML file\nprofile.to_file('bankloans_report.html')","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:34:55.865165Z","iopub.execute_input":"2024-01-11T16:34:55.865577Z","iopub.status.idle":"2024-01-11T16:35:27.589058Z","shell.execute_reply.started":"2024-01-11T16:34:55.865544Z","shell.execute_reply":"2024-01-11T16:35:27.587468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.592294Z","iopub.execute_input":"2024-01-11T16:35:27.592886Z","iopub.status.idle":"2024-01-11T16:35:27.611886Z","shell.execute_reply.started":"2024-01-11T16:35:27.592848Z","shell.execute_reply":"2024-01-11T16:35:27.609889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.613724Z","iopub.execute_input":"2024-01-11T16:35:27.614151Z","iopub.status.idle":"2024-01-11T16:35:27.626763Z","shell.execute_reply.started":"2024-01-11T16:35:27.614117Z","shell.execute_reply":"2024-01-11T16:35:27.625535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.628352Z","iopub.execute_input":"2024-01-11T16:35:27.628867Z","iopub.status.idle":"2024-01-11T16:35:27.671414Z","shell.execute_reply.started":"2024-01-11T16:35:27.628833Z","shell.execute_reply":"2024-01-11T16:35:27.669811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Data Cleaning**\n\n#### Step 1: Define Predictor Variables\nDefine the predictor variables (features) to be used in the model. In this case, we are using age, education, employment, address, income, debt-to-income ratio, credit debt, and other debts.\n\n#### Step 2: Split Dataset for Missing Target Imputation\n\nSplit the dataset into two subsets: \n- A subset with non-missing target values (`subset_with_target`).\n- A subset with missing target values (`subset_missing_target`).\n\n#### Step 3: Train/Test Split for Imputation Model\n\nPerform a train/test split on the subset with the target variable. This split is used for training and evaluating the logistic regression model for imputation.\n\n#### Step 4: Logistic Regression for Imputation\n\nTrain a logistic regression model on the training set to predict the missing target values.\n\n#### Step 5: Impute Missing Target Values\n\nUse the trained logistic regression model to predict and impute the missing target values in `subset_missing_target`.\n\n#### Step 6: Combine Imputed Data\n\nCombine the subset with imputed values and the subset with original target values into a single DataFrame, `df_imputed`.\n\n#### Step 7: Evaluate Imputation Model Performance\n\nEvaluate the accuracy of the imputation model using the test set.\n\n#### Step 8: Data Preparation for Machine Learning\n\nPrepare the data for machine learning by separating features (X) and the target variable (y) after imputation.\n\n#### Step 9: Apply SMOTE for Class Balancing\n\nUse SMOTE (Synthetic Minority Over-sampling Technique) to balance the classes in the target variable.\n\n#### Step 10: Check Class Distribution After Oversampling\n\nCheck the distribution of classes in the target variable after applying SMOTE.\n\n#### Step 11: Create Combined DataFrame\n\nCreate a new DataFrame, `df_combined`, that includes both the imputed values and the oversampled data.\n\nThis structure provides a clear, step-by-step breakdown of the preprocessing and modeling steps, ideal for documentation or educational purposes in a Markdown format.","metadata":{}},{"cell_type":"code","source":"# Define the predictor variables (features)\npredictor_columns = ['age', 'ed', 'employ', 'address', 'income', 'debtinc', 'creddebt', 'othdebt']  # Replace with your actual feature names\n\n# Split the dataset into two subsets: one with non-missing target values and one with missing target values\nsubset_with_target = df[df['default'].notna()]\nsubset_missing_target = df[df['default'].isna()]\n\n# Split the subset_with_target into train and test sets for model training and evaluation\nX_train, X_test, y_train, y_test = train_test_split(\n    subset_with_target[predictor_columns],\n    subset_with_target['default'],\n    test_size=0.2,\n    random_state=42\n)\n\n# Train a logistic regression model for imputation\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n# Predict the missing target values in subset_missing_target\nmissing_target_predictions = model.predict(subset_missing_target[predictor_columns])\n\n# Impute the missing target values with the predicted values\nsubset_missing_target['default'] = missing_target_predictions\n\n# Combine the two subsets back into the original dataset\ndf_imputed = pd.concat([subset_with_target, subset_missing_target])\n\n# Evaluate the performance of the imputation model\nimputation_accuracy = accuracy_score(y_test, model.predict(X_test))\nprint(f\"Imputation Model Accuracy: {imputation_accuracy:.2f}\")\n\n# Separate the features (X) and the target variable (y) after imputation\nX = df_imputed.drop('default', axis=1)\ny = df_imputed['default']\n\n# Apply SMOTE to balance the target variable\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\n# Check the class distribution after oversampling\nclass_distribution_after = y_resampled.value_counts()\nprint(\"\\nClass distribution after oversampling:\")\nprint(class_distribution_after)\n\n# Create a new DataFrame with imputed values and oversampled data\ndf_combined = pd.DataFrame(X_resampled, columns=X.columns)\ndf_combined['default'] = y_resampled","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.673532Z","iopub.execute_input":"2024-01-11T16:35:27.674025Z","iopub.status.idle":"2024-01-11T16:35:27.735430Z","shell.execute_reply.started":"2024-01-11T16:35:27.673990Z","shell.execute_reply":"2024-01-11T16:35:27.733978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now df_combined contains the combined results with imputed values and oversampled data\n","metadata":{}},{"cell_type":"code","source":"# Check Balanced Status\ndf_combined['default'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.737258Z","iopub.execute_input":"2024-01-11T16:35:27.737586Z","iopub.status.idle":"2024-01-11T16:35:27.748285Z","shell.execute_reply.started":"2024-01-11T16:35:27.737558Z","shell.execute_reply":"2024-01-11T16:35:27.746496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check Duplicated Rows in our Data\ndf_combined.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.749865Z","iopub.execute_input":"2024-01-11T16:35:27.750379Z","iopub.status.idle":"2024-01-11T16:35:27.761416Z","shell.execute_reply.started":"2024-01-11T16:35:27.750348Z","shell.execute_reply":"2024-01-11T16:35:27.759984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Outlier Detection and Explanation","metadata":{}},{"cell_type":"markdown","source":"Here's a brief analysis of potential outliers, based on the box plots:\n\n1. **Age**: There appear to be no significant outliers in both default groups.\n2. **Education (ed)**: Some high education levels are outliers in both default groups.\n3. **Employment (employ)**: A few cases with exceptionally high employment years, especially in the non-default group.\n4. **Address**: Similar to employment, a few high values, more pronounced in the non-default group.\n5. **Income**: Notable outliers with very high incomes, especially in the non-default group.\n6. **Debt-to-Income Ratio (debtinc)**: Outliers present with very high ratios in both groups.\n7. **Credit Debt (creddebt) and Other Debt (othdebt)**: Both have outliers with very high debt amounts in both default groups.","metadata":{}},{"cell_type":"code","source":"bp = PdfPages('BoxPlots with default Split.pdf')\n\nfor num_variable in df_combined.columns:\n    fig,axes = plt.subplots(figsize=(10,4))\n    sns.boxplot(x='default', y=num_variable, data = df_combined)\n    bp.savefig(fig)\nbp.close()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:27.765717Z","iopub.execute_input":"2024-01-11T16:35:27.766654Z","iopub.status.idle":"2024-01-11T16:35:29.884859Z","shell.execute_reply.started":"2024-01-11T16:35:27.766604Z","shell.execute_reply":"2024-01-11T16:35:29.883544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Deal with Outliers\n**First**: Make IQR and Log_transform Functions to deal with each Feature in Our Data","metadata":{}},{"cell_type":"code","source":"# Function to cap outliers using the IQR method\ndef cap_outliers_iqr(series):\n    Q1 = series.quantile(0.25)\n    Q3 = series.quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    return series.clip(lower_bound, upper_bound)\n\n# Function for log transformation\ndef log_transform(series):\n    return np.log(series + 1)  # Adding 1 to avoid log(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:29.886354Z","iopub.execute_input":"2024-01-11T16:35:29.886943Z","iopub.status.idle":"2024-01-11T16:35:29.893890Z","shell.execute_reply.started":"2024-01-11T16:35:29.886910Z","shell.execute_reply":"2024-01-11T16:35:29.892156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply Changes to each feature that have Outliers","metadata":{}},{"cell_type":"code","source":"# Employment (employ) and Address: Cap outliers\ndf_combined['employ'] = cap_outliers_iqr(df_combined['employ'])\ndf_combined['address'] = cap_outliers_iqr(df_combined['address'])\n\n# Income: Apply log transformation\ndf_combined['income'] = log_transform(df_combined['income'])\n\n# Debt-to-Income Ratio (debtinc): Apply log transformation\ndf_combined['debtinc'] = log_transform(df_combined['debtinc'])\n\n# Credit Debt (creddebt) and Other Debt (othdebt): Apply log transformation\ndf_combined['creddebt'] = log_transform(df_combined['creddebt'])\ndf_combined['othdebt'] = log_transform(df_combined['othdebt'])","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:29.895116Z","iopub.execute_input":"2024-01-11T16:35:29.895445Z","iopub.status.idle":"2024-01-11T16:35:29.920559Z","shell.execute_reply.started":"2024-01-11T16:35:29.895415Z","shell.execute_reply":"2024-01-11T16:35:29.918941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_combined.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:29.922351Z","iopub.execute_input":"2024-01-11T16:35:29.923060Z","iopub.status.idle":"2024-01-11T16:35:29.937809Z","shell.execute_reply.started":"2024-01-11T16:35:29.923022Z","shell.execute_reply":"2024-01-11T16:35:29.936826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"---\n# Data Visualization","metadata":{}},{"cell_type":"markdown","source":"### 1. Histogram of Ages","metadata":{}},{"cell_type":"code","source":"# Histogram of Ages\nhist_age = px.histogram(df_combined, x='age', title='Age Distribution')\nhist_age.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:29.939214Z","iopub.execute_input":"2024-01-11T16:35:29.939575Z","iopub.status.idle":"2024-01-11T16:35:32.662505Z","shell.execute_reply.started":"2024-01-11T16:35:29.939543Z","shell.execute_reply":"2024-01-11T16:35:32.660987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2. Histogram of creddebt","metadata":{}},{"cell_type":"code","source":"hist_age = px.histogram(df_combined, x='creddebt', title='Cred Debt Distribution')\nhist_age.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:32.663873Z","iopub.execute_input":"2024-01-11T16:35:32.664188Z","iopub.status.idle":"2024-01-11T16:35:32.735603Z","shell.execute_reply.started":"2024-01-11T16:35:32.664161Z","shell.execute_reply":"2024-01-11T16:35:32.734505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. Box Plot for Income by Default Status","metadata":{}},{"cell_type":"code","source":"# Box plot for Income by Default Status\nbox_income = px.box(df_combined, x='default', y='income', \n                    labels={\"default\": \"Default Status\", \"income\": \"Income\"},\n                    title='Income Distribution by Default Status')\nbox_income.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:32.737007Z","iopub.execute_input":"2024-01-11T16:35:32.737348Z","iopub.status.idle":"2024-01-11T16:35:32.842840Z","shell.execute_reply.started":"2024-01-11T16:35:32.737319Z","shell.execute_reply":"2024-01-11T16:35:32.841250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Scatter Plot of Income vs. Debt-to-Income Ratio\n### In another Way of Analysis: What is the relationship between income and credit card debt?\nThis question examines if higher income individuals tend to have more credit card debt.\n\n","metadata":{}},{"cell_type":"code","source":"# Scatter plot of Income vs. Debt-to-Income Ratio\nscatter_debt_income = px.scatter(df_combined, x='income', y='debtinc', color='default', \n                                 labels={\"income\": \"Income\", \"debtinc\": \"Debt-to-Income Ratio\"},\n                                 title='Income vs. Debt-to-Income Ratio by Default Status')\nscatter_debt_income.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:32.844482Z","iopub.execute_input":"2024-01-11T16:35:32.845379Z","iopub.status.idle":"2024-01-11T16:35:32.987706Z","shell.execute_reply.started":"2024-01-11T16:35:32.845333Z","shell.execute_reply":"2024-01-11T16:35:32.986651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Bar Plot for Average Income by Education Level","metadata":{}},{"cell_type":"code","source":"# Bar plot for Average Income by Education Level\nbar_edu_income = px.bar(df_combined.groupby('ed')['income'].mean().reset_index(), \n                        x='ed', y='income', \n                        labels={\"ed\": \"Education Level\", \"income\": \"Average Income\"},\n                        title='Average Income by Education Level')\nbar_edu_income.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:32.989102Z","iopub.execute_input":"2024-01-11T16:35:32.990939Z","iopub.status.idle":"2024-01-11T16:35:33.084572Z","shell.execute_reply.started":"2024-01-11T16:35:32.990893Z","shell.execute_reply":"2024-01-11T16:35:33.083460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Pie Chart Showing the Proportion of Default vs. Non-Default Cases","metadata":{}},{"cell_type":"code","source":"# Pie chart for Proportion of Default vs Non-Default Cases\npie_default = px.pie(df_combined, names='default', \n                     title='Proportion of Default vs Non-Default Cases',\n                     labels={\"default\": \"Default Status\"})\npie_default.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.086246Z","iopub.execute_input":"2024-01-11T16:35:33.086948Z","iopub.status.idle":"2024-01-11T16:35:33.168964Z","shell.execute_reply.started":"2024-01-11T16:35:33.086906Z","shell.execute_reply":"2024-01-11T16:35:33.167666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pie chart for Proportion of Default vs Non-Default Cases\npie_default = px.pie(df_combined, names='ed', \n                     title='Proportion of Education Level',\n                     labels={\"ed\": \"Education Level\"})\npie_default.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.170517Z","iopub.execute_input":"2024-01-11T16:35:33.170999Z","iopub.status.idle":"2024-01-11T16:35:33.234336Z","shell.execute_reply.started":"2024-01-11T16:35:33.170960Z","shell.execute_reply":"2024-01-11T16:35:33.233102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Line plot for Average Debt-to-Income Ratio by Years of Employment\nline_emp_debtinc = px.line(df_combined.groupby('employ')['debtinc'].mean().reset_index(), \n                           x='employ', y='debtinc', \n                           labels={\"employ\": \"Years of Employment\", \"debtinc\": \"Average Debt-to-Income Ratio\"},\n                           title='Average Debt-to-Income Ratio by Years of Employment')\nline_emp_debtinc.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.235721Z","iopub.execute_input":"2024-01-11T16:35:33.236335Z","iopub.status.idle":"2024-01-11T16:35:33.342566Z","shell.execute_reply.started":"2024-01-11T16:35:33.236297Z","shell.execute_reply":"2024-01-11T16:35:33.340968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Analysis","metadata":{}},{"cell_type":"markdown","source":"### 1. What is the average age of individuals who have defaulted on a loan versus those who have not?\n\nThis question aims to understand if there's a noticeable age difference between those who default and those who don't.","metadata":{}},{"cell_type":"code","source":"# 1. Average age of individuals who have defaulted vs. those who have not\navg_age_default = df_combined.groupby('default')['age'].mean()\nprint(\"Average Age by Default Status\", avg_age_default)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.344065Z","iopub.execute_input":"2024-01-11T16:35:33.345182Z","iopub.status.idle":"2024-01-11T16:35:33.357381Z","shell.execute_reply.started":"2024-01-11T16:35:33.345126Z","shell.execute_reply":"2024-01-11T16:35:33.355682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Average Age of Individuals Who Have Defaulted vs. Those Who Have Not**:\n   - Non-defaulters: Average age is approximately 34.3 years.\n   - Defaulters: Average age is approximately 31.4 years.\n   This suggests that younger individuals are slightly more likely to default.","metadata":{}},{"cell_type":"markdown","source":"### 2. Is there a correlation between the level of education and the likelihood of defaulting on a loan?\nThis explores if education level impacts loan defaulting.","metadata":{}},{"cell_type":"code","source":"# 2. Correlation between education level and defaulting\ndefault_rate_by_edu = df_combined.groupby('ed')['default'].mean()\nprint(\"Default Rate by Education Level\", default_rate_by_edu)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.359530Z","iopub.execute_input":"2024-01-11T16:35:33.360015Z","iopub.status.idle":"2024-01-11T16:35:33.372542Z","shell.execute_reply.started":"2024-01-11T16:35:33.359971Z","shell.execute_reply":"2024-01-11T16:35:33.371723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation Between Education Level and Defaulting**:\n   - Education Level 1: 21.2% default rate.\n   - Education Level 2: 29.8% default rate.\n   - Education Level 3: 34.5% default rate.\n   Higher education levels (2 and 3) have a higher default rate, except for level 1, which has a lower default rate, though this could be due to a smaller sample size for level 1.","metadata":{}},{"cell_type":"markdown","source":"### 3. How does the length of employment affect the debt-to-income ratio?\nInvestigates if longer employment correlates with a better or worse debt-to-income ratio.","metadata":{}},{"cell_type":"code","source":"# 3. Relationship between length of employment and debt-to-income ratio\navg_debtinc_by_employ = df_combined.groupby('employ')['debtinc'].mean()\nprint(\"Average Debt-to-Income Ratio by Employment Years\", avg_debtinc_by_employ)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.373901Z","iopub.execute_input":"2024-01-11T16:35:33.374548Z","iopub.status.idle":"2024-01-11T16:35:33.394914Z","shell.execute_reply.started":"2024-01-11T16:35:33.374508Z","shell.execute_reply":"2024-01-11T16:35:33.393921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Average Debt-to-Income Ratio by Employment Years**:\n   - The debt-to-income ratio varies with years of employment, peaking at around 10-15 years of employment and generally decreasing afterwards. This suggests that individuals in mid-career stages might have higher debt burdens relative to their income.","metadata":{}},{"cell_type":"markdown","source":"### 4. Are individuals with higher debts more likely to default?\nA key question for risk assessment in lending.","metadata":{}},{"cell_type":"code","source":"# 4. Average debts of individuals who default vs. those who do not\navg_debts_by_default_status = df_combined.groupby('default')[['creddebt', 'othdebt']].mean()\nprint(\"Average Debts by Default Status\\n\", avg_debts_by_default_status)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:33.399253Z","iopub.execute_input":"2024-01-11T16:35:33.400165Z","iopub.status.idle":"2024-01-11T16:35:33.410928Z","shell.execute_reply.started":"2024-01-11T16:35:33.400126Z","shell.execute_reply":"2024-01-11T16:35:33.409884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Average Debts of Individuals Who Default vs. Those Who Do Not**:\n- **Non-defaulters**:\n  - Average credit card debt: Approximately 0.90\n  - Average other debts: Approximately 2.05\n\n- **Defaulters**:\n  - Average credit card debt: Approximately 1.34\n  - Average other debts: Approximately 2.61","metadata":{}},{"cell_type":"markdown","source":"---\n# Feature Engineering\nhere are some feature engineering steps we could consider:\n\n1. **Binning Age**: Group ages into categories such as 'Young', 'Middle-Aged', 'Senior'. This can sometimes help models capture nonlinear relationships better.\n\n2. **Log Transformation**: Apply log transformation to skewed data like income, creddebt, and othdebt to normalize their distribution. Many models work better with normally distributed data.\n\n3. **Interaction Terms**: Create interaction terms that are combinations of existing features, such as `income * debtinc` to represent the interaction between income and debt-to-income ratio.\n\n4. **One-Hot Encoding for Education Level**: Convert the education level (ed) into dummy variables for each level of education. This is necessary since most machine learning models require numerical input.\n\n5. **Standardizing or Normalizing Data**: Standardize (or normalize) features like income, debtinc, etc., to have a mean of 0 and a standard deviation of 1 (or to range between 0 and 1). This is particularly important for algorithms that are sensitive to the scale of the data, like SVM or k-NN.\n\n6. **Default History**: If you have historical data, creating a feature that counts the number of previous defaults can be useful.\n\n7. **Debt Ratios**: Create new features like the ratio of creddebt to income or othdebt to income, providing a more nuanced view of debt relative to income.\n\n8. **Employment Stability**: A feature that combines years of employment and years at the current address could indicate overall stability, which might be predictive of loan repayment.\n","metadata":{}},{"cell_type":"code","source":"# Copy the original dataset\ndata_fe = df_combined.copy()\n\n# 1. Binning Age\nbins = [0, 25, 40, 60, 100]\nlabels = ['Young', 'Young Adult', 'Middle-Aged', 'Senior']\ndata_fe['age_bin'] = pd.cut(data_fe['age'], bins=bins, labels=labels)\n\n# 2. Log Transformation\nfor col in ['income', 'creddebt', 'othdebt']:\n    data_fe[col + '_log'] = np.log(data_fe[col] + 1)  # Adding 1 to avoid log(0)\n\n# 3. Interaction Terms\ndata_fe['income_debtinc_interaction'] = data_fe['income'] * data_fe['debtinc']\n\n# 4. One-Hot Encoding for Education Level\nedu_dummies = pd.get_dummies(data_fe['ed'], prefix='ed')\ndata_fe = pd.concat([data_fe, edu_dummies], axis=1)\n\n# 5. Standardizing Data\nscaler = StandardScaler()\nscaled_cols = ['income', 'debtinc', 'creddebt', 'othdebt']\ndata_fe[scaled_cols] = scaler.fit_transform(data_fe[scaled_cols])\n\n# 6. Debt Ratios\ndata_fe['creddebt_income_ratio'] = data_fe['creddebt'] / data_fe['income']\ndata_fe['othdebt_income_ratio'] = data_fe['othdebt'] / data_fe['income']\n\n# Show the first few rows of the modified dataset\ndata_fe.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:35.235006Z","iopub.execute_input":"2024-01-11T16:35:35.235389Z","iopub.status.idle":"2024-01-11T16:35:35.286991Z","shell.execute_reply.started":"2024-01-11T16:35:35.235359Z","shell.execute_reply":"2024-01-11T16:35:35.285488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset has been enhanced with several new features, making it more suitable for machine learning analysis:\n\n1. **Age Binned**: Ages have been categorized into 'Young', 'Young Adult', 'Middle-Aged', and 'Senior'.\n\n2. **Log Transformation**: Applied to 'income', 'creddebt', and 'othdebt' to reduce skewness.\n\n3. **Interaction Term**: Created between 'income' and 'debtinc', capturing their combined effect.\n\n4. **One-Hot Encoding for Education**: 'ed' is transformed into dummy variables for each education level.\n\n5. **Standardization**: 'income', 'debtinc', 'creddebt', and 'othdebt' have been scaled.\n\n6. **Debt Ratios**: New features 'creddebt_income_ratio' and 'othdebt_income_ratio' show debts relative to income.","metadata":{}},{"cell_type":"code","source":"data_fe.info()","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:35:36.923532Z","iopub.execute_input":"2024-01-11T16:35:36.924309Z","iopub.status.idle":"2024-01-11T16:35:36.941220Z","shell.execute_reply.started":"2024-01-11T16:35:36.924264Z","shell.execute_reply":"2024-01-11T16:35:36.939638Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning Baseline**","metadata":{}},{"cell_type":"markdown","source":"#### **Step 1:** Data Preparation\n\nPrepare the dataset for machine learning. This involves selecting the features (X) and the target variable (y). Assume `data_fe` is the dataset with feature engineering already applied.","metadata":{}},{"cell_type":"code","source":"# ['age', 'ed', 'age_bin', 'default']\nX = data_fe.drop('default', axis=1)\ny = data_fe['default']","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:52:14.432921Z","iopub.execute_input":"2024-01-11T16:52:14.433966Z","iopub.status.idle":"2024-01-11T16:52:14.442408Z","shell.execute_reply.started":"2024-01-11T16:52:14.433919Z","shell.execute_reply":"2024-01-11T16:52:14.440783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 2:** Train/Test Split\n\nSplit the dataset into training and testing sets. This is crucial for training the model and for an unbiased evaluation of its performance.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:52:48.846314Z","iopub.execute_input":"2024-01-11T16:52:48.846721Z","iopub.status.idle":"2024-01-11T16:52:48.856367Z","shell.execute_reply.started":"2024-01-11T16:52:48.846689Z","shell.execute_reply":"2024-01-11T16:52:48.855023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 4**: Create XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"xgb_classifier = xgb.XGBClassifier(random_state=42, enable_categorical=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:52:49.518058Z","iopub.execute_input":"2024-01-11T16:52:49.518928Z","iopub.status.idle":"2024-01-11T16:52:49.525132Z","shell.execute_reply.started":"2024-01-11T16:52:49.518890Z","shell.execute_reply":"2024-01-11T16:52:49.523690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 5:** Train the Classifier","metadata":{}},{"cell_type":"code","source":"xgb_classifier.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:52:51.361376Z","iopub.execute_input":"2024-01-11T16:52:51.361830Z","iopub.status.idle":"2024-01-11T16:52:51.469370Z","shell.execute_reply.started":"2024-01-11T16:52:51.361790Z","shell.execute_reply":"2024-01-11T16:52:51.468426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 6:** Make Predictions and Evaluate the Model\nEvaluate the model's performance using the confusion matrix and classification report. These metrics provide insights into the accuracy, precision, recall, and F1-score of the model.","metadata":{}},{"cell_type":"code","source":"y_pred = xgb_classifier.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, average='binary') # Change 'binary' to 'macro' or 'weighted' for multi-class\nrecall = recall_score(y_test, y_pred, average='binary') # Change 'binary' to 'macro' or 'weighted' for multi-class\nf1 = f1_score(y_test, y_pred, average='binary') # Change 'binary' to 'macro' or 'weighted' for multi-class\n\nprint(f\"Accuracy: {accuracy:.2f}\")\nprint(f\"Precision: {precision:.2f}\")\nprint(f\"Recall: {recall:.2f}\")\nprint(f\"F1 Score: {f1:.2f}\")\n\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-01-11T16:52:54.839490Z","iopub.execute_input":"2024-01-11T16:52:54.839898Z","iopub.status.idle":"2024-01-11T16:52:54.875635Z","shell.execute_reply.started":"2024-01-11T16:52:54.839869Z","shell.execute_reply":"2024-01-11T16:52:54.874475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Model Improvement**\n\n#### **Step 1**: Define Parameter Grid\nSpecify the range of hyperparameters to test. Here's an example grid:\n","metadata":{}},{"cell_type":"code","source":"param_grid = {\n    'n_estimators': [100, 200, 300],\n    'learning_rate': [0.01, 0.05, 0.1]\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-10T13:37:33.077604Z","iopub.execute_input":"2024-01-10T13:37:33.078070Z","iopub.status.idle":"2024-01-10T13:37:33.084492Z","shell.execute_reply.started":"2024-01-10T13:37:33.078017Z","shell.execute_reply":"2024-01-10T13:37:33.083215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 2**: Configure Grid Search CV\nInstantiate GridSearchCV with the Random Forest Classifier, parameter grid, and the number of folds for cross-validation. For example, using 5-fold cross-validation:","metadata":{}},{"cell_type":"code","source":"grid_search = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T13:37:34.333629Z","iopub.execute_input":"2024-01-10T13:37:34.334012Z","iopub.status.idle":"2024-01-10T13:37:34.339705Z","shell.execute_reply.started":"2024-01-10T13:37:34.333983Z","shell.execute_reply":"2024-01-10T13:37:34.338805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 3:** Fit Grid Search to Data\nUse the training data for this step.\n\n","metadata":{}},{"cell_type":"code","source":"grid_search.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-10T13:37:34.969096Z","iopub.execute_input":"2024-01-10T13:37:34.969817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 4:** View Best Parameters\nAfter fitting, you can find the best parameters from the grid search.\n","metadata":{}},{"cell_type":"code","source":"best_params = grid_search.best_params_\nprint(f\"Best parameters: {best_params}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **Step 5:** Evaluate the Best Model\nUse the best estimator to make predictions and evaluate its performance.","metadata":{}},{"cell_type":"code","source":"best_model = grid_search.best_estimator_\n\n# Evaluate the best model\ny_pred_best = best_model.predict(X_test)\nprint(\"Best Model Accuracy:\", accuracy_score(y_test, y_pred_best))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Author: Abdelrahman Ashour","metadata":{}}]}